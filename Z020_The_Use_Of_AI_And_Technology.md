---
title: 28. The Use of AI and Technology
layout: default
nav_order: 30
---
# The Use of AI and Technology

### [Previous Chapter](Z019_Money_Budget_Management.html)

## **Short Answer**

**The problems you encounter as a teacher don't often change, but the means (and technology) to address those problems do often change.**

AGI (Artificial general intelligence) is possible. It may take a while to get there, however.

Opinion: The public proliferation of AI in, well just about every field and occupation, is the equivalent of letting the average person, often with no training, pilot an A10 Thunderbolt II from Fairchild Republic with a GAU-8/A Avenger from General Electric (or General Dynamics) strapped onto it. It's overkill and not appropriate for many situations, costs a lot to use and deploy, and it is extremely easy to cause unintentional and collateral damage with if handled incorrectly... *but it does neutralize problems.* There are valuable uses for AI, but there's just as many uses which aren't practical or even make sense.
- Automation in history is usually adopted because it was far more effective *and* efficient compared to current methods to solve a problem. If the invention isn't *both* effective and efficient, it may not persist for long or cause detrimental damage to its users over time.
- The biggest "hidden cost" is ambiguity and accidental complexity.
- Other examples of overkill: giving frying pans and coffee machines firmware updates and AI.

For a viewpoint on AI centralized primarily on education, I'd reference the work of Anne Lutz Fernandez, [specifically her *Help Sheet: Resisting AI Mania in Schools* in Google Documents](https://docs.google.com/document/d/1n9CokRz8xRR-sO01DIVkuftFywxSay6ae5eLf__UYJM/edit?usp=sharing), as I'll share some viewpoints with them (2025). 

As for technology, we have enough tools to do things we never could've done before in the past, but we also lose the nuance for *how* things work "under the hood." There's also a lot of ways to abuse, exploit, and use technology, despite whatever guardrails you may implement.
- E.g. Chances are you probably don't know how to change over half the settings on a smart phone do, even if you're someone with one (almost) always on your person.

## **Long Answer**

### **A Technology Overview in Education**

Technology is permeating into classrooms at primary, secondary, tertiary, post-graduate, and adult education. This has accelerated greatly since COVID back in 2020. Schools may move towards near-full or full technological equivalents for resources like accessing textbooks and generating work for assignments.
- Whether or not this becomes a system administrator's nightmare in the future remains to be seen.
- Though technology may intend to make education accessible, it can also hinder learning.

To give an example: a high school now provides every student with their own personal iPad (electronic tablet). It may have the following features:
- Specifications to run school required software at acceptable parameters
- An internet browser
- Camera software
- A monitoring software/setup in the backend so a teacher can monitor what's on a student's screen in the background during their specific class period
    - These softwares typically just project what's actively showing on one screen to a dashboard/another screen
    - In this example, it's also the *school's* computer, not the student's. The school may do this.
- Proprietary security system
- Email
- Assignment and work software
- Activity software
- Storage and organization software
- Classroom management software
- "All-in-one" software to cover multiple use cases
- Access to online textbooks
- An insurance/warranty plan in case of damages

A student iPad can cover almost all use cases in a school setting and simplify the logistics of resource management. It's also a trade-off on resources and reduction in capacity/lot-sizing; though you can now manage fewer physical resources, providing technology may require additional overhead like licensing, device management systems, connectivity, internet providers, and equity concerns.

Not *all* technology is great or necessary as preferences still exist. People may vastly prefer a physical medium to doing work vs an electronic medium. It may also be an accommodation to provide a physical medium vs an electronic medium. There's also cases where certain activities cannot be substituted with technology (currently), so it still requires physical resources.

### **Technology and Security**

Technology, like computers and cell phones, can also be misused by students *and* staff as well, including jailbreaking, installing unauthorized software, and damaging hardware. The human element is typically the easiest way to bypass any technology's security system to cripple it.
- Example 1: A post-it note with your password on it next to a shared computer.
- Example 2: Sharing your password with someone else so they can use your device or a school provided device in general.
- Example 3: Keeping the default settings, which are easy to find out about, for a username/password to an administrator account.
- Example 4: Making "12345" your combination key.
- Example 5: You might also be [working at the Louvre and have the password set to "Louvre" for your surveillance system](https://abcnews.go.com/International/password-louvres-video-surveillance-system-louvre-employee/story?id=127236297) (Leath & Geho, 2025). 
    - While that's easy to remember, that's just as easy to guess and cause havoc with.

There's also more points to consider with technology:
- User privileges (e.g. students and staff) are deliberately limited to mitigate permanent damage through software in devices vs administrator privileges (e.g. IT teams).
- Unauthorized software may violate licenses and school policies, introducing more legal trouble.
- Damages may lead to financial responsibility and paying back the school for losses.
- Malware and insecure software on one system can easily and quickly spread to every other connected system in a network.

Though technological misuse can be mitigated through proper training, humans still remain one of the easiest, if not *the* easiest, ways to cause damage to technology in any setting. This applies to schools, homes, businesses, and many more places.

As an example of human engineering above: there's a possibility you have security systems up-to-date and ready to defend against intrusions, but something like an employee's computer got breached and now there's an open door into every other system ripe for the taking.

To dilute technological security (cybersecurity) down into two points:
1. Technology, and its security, is far more protected with better protocols and tools compared to the past. Said protocols are continuously improving over time.
2. At the same time, how much damage you can do with technology makes people extremely vulnerable compared to what was possible in the past.

### **AI in General**

Most AI use in education is *doing the work for someone else* (not to be confused with having the work done). If a student knows the content, they should not *need* AI to explain, defend, or interpret the content. Students doing the appropriate practice and work on their own to achieve mastery is part of the learning process. Without it, critical thinking and other important skills suffer.
- For example: the text you're reading now. AI could generate an answer for everything here, but I know the material here because I did the human element, which is the work I put in, *and wrote the thing in the first place.* The act of going through the work reinforced my knowledge, which is crucial for learning.

If you combine a substitute of mastery with a society diminishing the value of honesty, integrity, and personal responsibility, humanity's educational level and and ability to accomplish tasks also diminishes.

**Here are some questions for you:**

- "When should you use AI?"
- "When should you NOT use AI?"
- "How do you know what AI generates is true and correct?"
- "When can you use a simpler method, like linear regression, instead of an advanced AI tool to solve your problem?"
- "At what point will AI fail to solve your problem(s)?"
- "Will using AI benefit your instruction?"

**If you cannot confidently answer these questions, that's OK, but I will tell you that you shouldn't use AI yet. If you do have solid answers, that's good.**

For those with a math or science background unsure where to start or new to AI, I'll point a finger at [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course) on AI/LLMs (Google Developers, 2025).
- If you're curious about how AI Detectors may work, focus on topics under the "Classification" section to help figure that out.

For non-technical readers, here's AI in plainer terms:

- Artificial Intelligence (AI) takes data and information, tries to find patterns and relationships within it based on prior knowledge (i.e., what it's trained on), and generates results from its findings.

Before continuing, I'll emphasize important points about AI that mirror my views on it (Cybersecurity and Infrastructure Security Agency [CISA], 2024).

- No matter how AI is used, you're responsible for the outcome. Always verify outputs.
- Don't feed it sensitive data, like medical records, legal documents, credit card statements, and/or confidential information in general. It can use, log, and transmit that data into a public space, and put you at risk of fraud, theft, and more.
- Even with its advantages, AI can remove learning opportunities and retention of information.
- Your students WILL use AI if able to. Make plans for its use.
- You don't always have to incorporate AI into a classroom. It's a tool, like any other.
- AI can hallucinate wrong answers with confidence.

That said, AI is a complicated topic. The velocity, volume, value, variety, and veracity of AI development increase year after year, month after month, and much of what I say could be invalidated within less than a year, if not 5 years. 
- For another nuanced take on AI, you can [refer to this blog post by Thomas Ptacek](https://fly.io/blog/youre-all-nuts/) (Warning: language) (Ptacek, 2025).

According to Simon Willison, on his Weblog in June 2025, the past six months (December 2024 to June 2025) *alone* there were multiple advancements in AI, including but not limited to (Willison, 2025):

- Amazon's Nova models
- DeepSeek V3
- Meta's Llama 3 Series
- Mistral Small 3
- Many, *many* more

### **The Hidden Complexity of AI**

When people typically use AI, at least in the years 2022-2025, they describe what they want and AI tries to do it.

This is where problems are far more noticeable, but not necessarily where problems start. It's also not just a case of "trash in, trash out" either.

Pretend you ask me, a human, to cook eggs for breakfast. I interpret the task as is and try to complete it. When you receive the eggs, they aren't what you were hoping they'd be.

*What went wrong?* The answer: accidental complexity.

Many things could be wrong. I could be an amateur chef who has never cooked eggs in their life before. I could also be a well-renowned chef who's cooked eggs tens of thousands, if not hundreds of thousands of times.

When someone typically delivers a task to AI, like Gemini or ChatGPT, they often send the request with the assumption said AI knows the context and underlying intent of the sender. Much like with asking it of humans, that isn't always the case. There's a high chance, if not certainty, deficiencies in knowledge are where processes and things are confidentally made up to try and reach a desired goal.

Let's go back to when you asked me to make eggs. You may not have, however, specified, or at least considered, several key requirements *essential* to this process, such as:
- How you want the eggs prepared
- What type of tools should I cook with
- Is the cooking equipment available for use
- What ingredients should I omit/use
- Is all the data organized (cleaned) properly or do I need to work off messy data

The lack of a one-size-fits-all, easy solution is intentional for many tasks. One example is where Fred Brooks describes essential complexity vs accidental complexity and states "the complexity of the design itself is essential" (1986). Think of accidental complexity as issues you could eliminate whereas essential complexity are issues you can only mitigate. Accidental complexity can take shape in many forms, such as:
- Lack of volume
- Data/knowledge infrastructure is messy or not organized
- Metrics of success, or what counts as success, is poorly defined
- Level of *ongoing* maintenance and cost required
- Processes not mapped out or mapped out ineffectively

To go back to eggs: essential complexity, in this case, is the system and interactions of finding, preparing, and serving the eggs for consumption. Accidental complexity is a countless array of other steps one could take to reach that same conclusion, such as substitutions, alternative cooking methods, and more. That complexity builds significantly faster when you automate a "generalist" to do everything rather than a "specialist" to do only a limited set of items. In all cases, automation complexity increases as the systems behind tasks in general also increase.

The very nature of cooking has complexity built into it no matter which steps you take. If you're careless, the entity doing the request will work based on what it knows, guess what it doesn't know, and try to meet requirements based on what you describe--whether the result is good or bad. It doesn't stop at just cooking either; it spans across myriad fields and situations.

### **AI in Instruction**

Where AI applies to teaching and students specifically is not likely to change easily. Here are some examples I've seen and would categorize below (The Institute for Ethical AI & ML, n.d.).

**Potentially OK AI Use**
- Repeating redundant tasks usually done manually, like grading
- Streamlining administrative and clerical work
- Generating concepts, examples, and ideas to create your solution from
- Compare submissions to answer keys for quick grading
- Students with disabilities are explicitly allowed to use it to help them learn
- To record and practice voice lines, which are transcribed into text
- To get out of a writer's block by doing creative collaboration

**Typically Prohibited AI Use**
- Completing assignments for students
- Feeding AI confidential or classified information
- As a replacement for learning new topics
- Using it as a substitute for critical thought and problem-solving
- Cheating
- Making AI videos of teachers/students (legal issues on this one)

As a stern reminder, anything listed under "Recommended AI Use" still requires a human to verify its accuracy and ensure that the outcome is what you intended.

### **How effective is AI really?**

**Part 1: Does it work well?**

In my opinion, as of November 2025, AI is not terribly effective (yet) and many expectations are overblown. About the only thing it can do *really* well is pattern based work, and even then it's still an unintelligent tool. AI cannot understand like a human can. Overdependence on AI by humans breeds normalized incompetence, which will be a rapidly growing problem in addition to its ever-growing costs as data complexity increases over time.
- Cleaning up messes is generally harder than preventing messes in the first place.

Another problem I see with AI is when people want to automate large projects or work at large scale. It is a resource sink in every way possible, whether through time, cost, staff, and management. The more things you try to automate, the resources required to create *and maintain* that automation exponentially increases. If you really need to automate something, then I'd say three things before attempting to do so:
1. Start small. Extremely small. Start with one tiny problem.
2. Assume whatever solution you make *will* cause problems and you need to maintain it for a long time.
3. Your operating costs may drastically increase with AI vs no AI.

I would say AI is better for people with prior domain expertise, as they can differentiate right and wrong and boost productivity with it. AI, however, is detrimental to those with lower background knowledge as it's more likely to create technial debt and these people may not accurately, or precisely, tell if outputs are good or bad.
- This aligns with views I've seen from other professionals in the technology sector, namely [Denis Stetskov on his post in September 25, 2025](https://techtrenches.substack.com/p/ai-wont-save-us-from-the-talent-crisis).

Despite that viewpoint, I wouldn't be surprised if someone told me people were *addicted* to AI, what it provides, and its capabilities. It's a technological marvel enabling the average user to interact with data and models with plain languages like English rather than coding languages like Python and R. It can solve problems in seconds what used to take some people days or longer to achieve. It can create life-saving medicines and discover methods humans may not normally achieve in their lifetimes.

All of this is to say AI is *extremely* powerful for someone who could've never done these things before without it, so it's no surprise to me sudden and easy access to it may cause attachment.

I'd also believe AI systems already have access to sensitive information and private security documents through user accounts, which should put any cybersecurity professionals on high alert right now. Many people unintentionally, or intentionally, insert private/legally protected information into an AI model, which allows it to train on that same information in the future. AI can be, and has been, utilized by humans as a means to manipulate, deceive, and attack other humans and resources, such as [the AI-powered PromptLock ransomware](https://www.welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/) (Cherepanov, 2025). I should further emphasize publicly/commercially available models are capable of all these feats, which opens up the number of people able to perform these acts.
- A seemingly innocent case may be using AI to parse content and generate summaries of a website on a search engine so you don't have to visit the website to learn about something.

There's also only so much data available to train an AI model on. A lot of available data is built upon centuries and millenia of prior information generated by people and translated into machine readable formats. It's entirely possible to "run out" of data to feed an AI to let it solve problems, which may inhibit its progress and slow improvements to its functions.

Views towards AI may also be distorted by adminstrative, managers, and directors thinking they can use AI to replace junior staff as well. That, however, means eliminating people you can train up to be seniors with domain expertise. If a company, nation, etc. invests so much into AI it replaces human labor, and by extension *paid* human labor for *free* AI labor, then how do humans afford goods and services or contribute to the economy?
- If you've seen Jurassic Park before, AI is the "could vs should" elephant in the room.

Lastly, AI is an umbrella for many types of automation and large language models (LLMs) are one item under that umbrella. While general AI may not be performing all that well, highly specialized machine learning tools dedicated to fields like astronomy, law, production, translation, and even medicine perform a select set of tasks and pattern recognition *exceedingly* well. Machine learning, while often lumped together with LLMs (large language models) and General AI (AGI), is extremely useful with real potential for improvements, but I'd still caution against overexaggerating its capabilities.

**Part 2: What about the cost of implementing/adding AI?**

In plain English, AI (and related systems like LLMs) have the following traits:
- AI frequency/usage can multiply costs like complexity does and accure *for each user request* in addition to any initial setup costs.
- Investment into depth or breadth alone is costly, even with simple or complex problems.
- Investing into *both* depth and breadth, or the ability to handle multiple areas well, is exponentially costly.
- Data complexity exponentially increases as well when more depth or breadth is introduced.
- The costs exist in both free and paid AI products consumers access.
- Despite any potential benefits, AI, LLMs, and automation in general can quickly force its designers/hosts into negative ROI (return on investment) and operate at a loss.

As for the *cost* of AI/automation, and who is responsible for owning said cost, that's probably one of three situations:
1. The user/employee directly incurs a monetary cost.
2. A technical and/or financial team/department, like IT or FinOps, manages the costs.
3. The head(s) of the organization manages the costs.

There could be other situations, but those three above seem the most likely in general. Nevertheless, because there is a *cost* someone, or something, needs to take responsibility for that cost or else you'll suffer from undue/unexpected expenses. If it remains unowned/unused, it's a resource to try and delete so it no longer drains monetary resources. If you need an effective, though crude, method to find who needs a resource, disable or remove it and wait until you find the person that complains the loudest.

Additionally, these costs aren't anything new or emerging with the rise of GenAI/AGI post-COVID or after 2020-2022. Automative engineering, since at least the 1990s, has exponential complexity as modern cars add more services, systems, and automated features. This means the numbers of interactions between all these new systems and services also increases inside cars (Walus, 2022).
- For any software engineers reading this, it's like dealing with at least O(n²) and O(2ⁿ) complexity as you're scaling systems.

I'll emphasize this part only talks about cost and complexity in automation. It doesn't cover whether risk increases with complexity, the breakeven points of complexity with task automation vs manual tasking, the Pareto efficiency of adding more automation features, and many more applications. I believe highlighting underlying costs is important as it may be overshadowed by all the benefits AI could provide.

You could also design your own AI instead of utilizing another AI service another provider has. Even if both you and the provider have the exact same models, parameters, and code, you may not have to pay high costs per use, but you may lack the sheer processing power and quality of life a provider can afford through more resources and technology backing their models.
- People may not even care if you can make a copy of a service for cheaper too, because it's not *that brand's product.* Name-brand recognition is very powerful.

### **Potential Solution(s) to AI in Class Settings**

As a reminder, AI can do some tasks really well and other tasks it flops down like a sad pancake on a griddle.

Still, if a AI program writes your essay or does an assignment in general for you, it's called cheating and academic dishonesty. You're claiming you did the work, despite another actually doing it for your instead.

It can also make 10 students give out the exact same answers, word-for-word, on their essays which are later reviewed and graded by a teacher. It does make homework and other assignments done outside-of-class much harder to verify the integrity of, but the quality is still questionable.

To counter this, a teacher may resort to doing in-person, handwritten tests at the school, in a monitored area, without the use of any assistive technology (i.e. paper and pencil). Accommodations may alleviate some of these restrictions, but likely not all of them to preserve educational integrity. If it's handwritten, there's also fewer techological issues and fewer excuses for why something isn't done.

It also means adapting how the classroom functions to get more work done *in* class instead of *outside* class. That may put an additional strain on teachers, but it does mitigate the issue of letting a student use AI to do the work for them.

I suppose if you want to try a different twist: you could create an assignment requiring AI usage, and declare cheating for those who did the assignment without AI.
- You could probably defend it by saying "the assignment asked for a specific software to complete it, but the student didn't use that software so they failed."

## **Bibliography**

1. Brooks, Frederick P. (1986). ["No Silver Bullet—Essence and Accident in Software Engineering"](https://www.cs.unc.edu/techreports/86-020.pdf) (PDF). Proceedings of the IFIP Tenth World Computing Conference: 1069–1076.

1. Cherepanov, Anton. (2025, August 26). *First known AI-powered ransomware uncovered by ESET Research.* Welivesecurity.com. [https://www.welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/](https://www.welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/)

1. Cybersecurity and Infrastructure Security Agency. (2024, September). *Secure our world: Using AI – Tip sheet*. [https://www.cisa.gov/sites/default/files/2024-09/Secure-Our-World-Using-AI-Tip-Sheet.pdf](https://www.cisa.gov/sites/default/files/2024-09/Secure-Our-World-Using-AI-Tip-Sheet.pdf)

2. Fernandez, A. L. (2025, November 25). *Help Sheet: Resisting AI Mania in Schools*. Google Documents. [https://docs.google.com/document/d/1n9CokRz8xRR-sO01DIVkuftFywxSay6ae5eLf__UYJM/edit?usp=sharing](https://docs.google.com/document/d/1n9CokRz8xRR-sO01DIVkuftFywxSay6ae5eLf__UYJM/edit?usp=sharing) 

2. Google Developers. (2025). *Machine learning crash course*. [https://developers.google.com/machine-learning/crash-course](https://developers.google.com/machine-learning/crash-course)

3. The Institute for Ethical AI & ML. (n.d.). *The responsible machine learning principles*. [https://ethical.institute/principles.html](https://ethical.institute/principles.html)

3. Leath, M., & Geho, L. (2025, November 6). *Password to Louvre’s video surveillance system was “Louvre”, according to employee.* ABC News. [https://abcnews.go.com/International/password-louvres-video-surveillance-system-louvre-employee/story?id=127236297](https://abcnews.go.com/International/password-louvres-video-surveillance-system-louvre-employee/story?id=127236297)

4. Ptacek, T. (2025, June 2). *My AI skeptic friends are all nuts*. Fly.io Blog. [https://fly.io/blog/youre-all-nuts/](https://fly.io/blog/youre-all-nuts/)

5. Stetskov, D. (2025, September 25). *AI Won’t Save Us From the Talent Crisis We Created.* Substack.com; From the Trenches. [https://techtrenches.substack.com/p/ai-wont-save-us-from-the-talent-crisis](https://techtrenches.substack.com/p/ai-wont-save-us-from-the-talent-crisis)

5. Walus, S. (2022, March 17). *How to deal with exponential complexity in automotive engineering*. Linkedin.com. [https://www.linkedin.com/pulse/how-deal-exponential-complexity-automotive-szymon-walus](https://www.linkedin.com/pulse/how-deal-exponential-complexity-automotive-szymon-walus)

5. Willison, S. (2025, June 6). *The last six months in LLMs, illustrated by pelicans on bicycles.* [https://simonwillison.net/2025/Jun/6/six-months-in-llms/](https://simonwillison.net/2025/Jun/6/six-months-in-llms/)

## **[Next Chapter](Z021_Design_Systems_EdTech.html)**